{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import os\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from pathlib import Path\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = \"2\"\n",
    "\n",
    "import cdt\n",
    "cdt.SETTINGS.rpath = \"/usr/bin/Rscript\"\n",
    "from cdt.causality.graph import PC\n",
    "\n",
    "import pandas as pd\n",
    "from src.features import preprocessing\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path('/home/ralmasri/projects/Thesis/Domain-Guided-Monitoring/data/')\n",
    "csv_path = data_path / \"logs_aggregated_concurrent.csv\"\n",
    "huawei_config = preprocessing.HuaweiPreprocessorConfig()\n",
    "huawei_config.aggregated_log_file = csv_path\n",
    "preprocessor = preprocessing.ConcurrentAggregatedLogsPreprocessor(huawei_config)\n",
    "# huawei_df = preprocessor._load_log_only_data().fillna(\"\")\n",
    "huawei_df = pd.read_csv(data_path / \"log_only_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sizes = [100, 1000, 10000, 50000]\n",
    "huawei_dfs = [huawei_df.head(x) for x in df_sizes]\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_columns = [\n",
    "            \"Hostname\",\n",
    "            \"log_level\",\n",
    "            \"programname\",\n",
    "            \"python_module\",\n",
    "            \"http_status\",\n",
    "            \"http_method\",\n",
    "            \"@timestamp\",\n",
    "            \"fine_log_cluster_template\",\n",
    "            \"coarse_log_cluster_template\",\n",
    "            \"url_cluster_template\"\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation without filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "EvDef = namedtuple('EvDef', ['type', 'value'])\n",
    "class EventDefinitionMap: # eid -> evdef\n",
    "    def __init__(self, top_dt, end_dt):\n",
    "        self.top_dt = top_dt\n",
    "        self.end_dt = end_dt\n",
    "        self._emap = {} # key : eid, val : evdef\n",
    "        self._ermap = {} # key : evdef, val : eid\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._emap)\n",
    "\n",
    "    def _eids(self):\n",
    "        return self._emap.keys()\n",
    "\n",
    "    def _next_eid(self):\n",
    "        eid = len(self._emap)\n",
    "        while eid in self._emap:\n",
    "            eid += 1\n",
    "        else:\n",
    "            return eid\n",
    "\n",
    "    def get_evdef(self, eid) -> EvDef:\n",
    "        return self._emap[eid]\n",
    "\n",
    "    def get_eid(self, evdef):\n",
    "        return self._ermap[evdef]\n",
    "\n",
    "    def process_row(self, columns, row):\n",
    "        row_eids = []\n",
    "        for column in columns:\n",
    "                if column == '@timestamp':\n",
    "                    continue\n",
    "                value = row[column]\n",
    "                if value == \"\":\n",
    "                    continue\n",
    "                d = {\n",
    "                    \"type\": column,\n",
    "                    \"value\": row[column],\n",
    "                }\n",
    "\n",
    "                evdef = EvDef(**d)\n",
    "\n",
    "                if evdef in self._ermap:\n",
    "                    row_eids.append(self._ermap[evdef])\n",
    "                else:\n",
    "                    eid = self._next_eid()\n",
    "                    self._emap[eid] = evdef\n",
    "                    self._ermap[evdef] = eid\n",
    "                    row_eids.append(eid)\n",
    "        return row_eids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label(dt_range, duration):\n",
    "    top_dt, end_dt = dt_range\n",
    "    l_label = []\n",
    "    temp_dt = top_dt\n",
    "    while temp_dt < end_dt:\n",
    "        l_label.append(temp_dt)\n",
    "        temp_dt += duration\n",
    "    l_label.append(end_dt)\n",
    "    return l_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize(l_dt, l_label, method):\n",
    "\n",
    "    def return_empty(size):\n",
    "        if method in (\"count\", \"binary\"):\n",
    "            return [0] * bin_num\n",
    "        elif method == \"datetime\":\n",
    "            return [[] for i in range(bin_num)]\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                \"Invalid method name ({0})\".format(method))\n",
    "\n",
    "    def init_tempobj():\n",
    "        if method == \"count\":\n",
    "            return 0\n",
    "        elif method == \"binary\":\n",
    "            return 0\n",
    "        elif method == \"datetime\":\n",
    "            return []\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                \"Invalid method name ({0})\".format(method))\n",
    "\n",
    "    def update_tempobj():\n",
    "        if method == \"count\":\n",
    "            return temp + 1\n",
    "        elif method == \"binary\":\n",
    "            return 1\n",
    "        elif method == \"datetime\":\n",
    "            temp.append(new_dt)\n",
    "            return temp\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                \"Invalid method name ({0})\".format(method))\n",
    "\n",
    "    bin_num = len(l_label) - 1\n",
    "    l_dt_temp = sorted(l_dt)\n",
    "    if len(l_dt_temp) <= 0:\n",
    "        return_empty(bin_num)\n",
    "\n",
    "    iterobj = iter(l_dt_temp)\n",
    "    try:\n",
    "        new_dt = next(iterobj)\n",
    "    except StopIteration:\n",
    "        raise ValueError(\"Not empty list, but failed to get initial value\")\n",
    "    while new_dt < l_label[0]:\n",
    "        try:\n",
    "            new_dt = next(iterobj)\n",
    "        except StopIteration:\n",
    "            return_empty(bin_num)\n",
    "\n",
    "    ret = []\n",
    "    stop = False\n",
    "    for label_dt in l_label[1:]:\n",
    "        temp = init_tempobj()\n",
    "        if stop:\n",
    "            ret.append(temp)\n",
    "            continue\n",
    "        while new_dt < label_dt:\n",
    "            temp = update_tempobj()\n",
    "            try:\n",
    "                new_dt = next(iterobj)\n",
    "            except StopIteration:\n",
    "                # \"stop\" make data after label term be ignored\n",
    "                stop = True\n",
    "                break\n",
    "        ret.append(temp)\n",
    "    return ret\n",
    "\n",
    "def autodiscretize_with_slide(l_dt, binsize, slide, dt_range):\n",
    "    top_dt, end_dt = dt_range\n",
    "    slide_width = max(int(binsize.total_seconds() / slide.total_seconds()), 1)\n",
    "    l_top = label((top_dt, end_dt), slide)[:-1]\n",
    "    l_end = [min(t + binsize, end_dt) for t in l_top]\n",
    "\n",
    "    ret = []\n",
    "    noslide = discretize(l_dt, l_top + [end_dt], method='datetime')\n",
    "\n",
    "    for i, bin_end in enumerate(l_end):\n",
    "        l_dt_temp = []\n",
    "        for b in noslide[i:i+slide_width]:\n",
    "            l_dt_temp.extend([dt for dt in b if dt <= bin_end])\n",
    "    \n",
    "        if len(l_dt_temp) > 0:\n",
    "            ret.append(1)\n",
    "        else:\n",
    "            ret.append(0)\n",
    "\n",
    "    return ret\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume we're using the G-squared test so everything is binary\n",
    "bin_overlap = datetime.timedelta(seconds = 8)\n",
    "\n",
    "def event2stat(evdict, top_dt, end_dt, dur):\n",
    "    \"\"\"This function looks at each event and returns a map that maps the \n",
    "    eid to a list that describes in which time bins the event occurs\"\"\"\n",
    "    d_stat = {}\n",
    "    labels = label((top_dt, end_dt), dur)\n",
    "    for eid, l_ev in evdict.items():\n",
    "        # if len(l_ev) == 0: # Skip events that dont have timestamps (shouldn't be possible)\n",
    "        #     continue\n",
    "\n",
    "        if bin_overlap == datetime.timedelta(seconds = 0):\n",
    "            val = discretize(l_ev, labels, method=\"binary\")\n",
    "        else:\n",
    "            slide = dur - bin_overlap\n",
    "            val = autodiscretize_with_slide(l_ev, dur, slide, dt_range = (top_dt, end_dt))\n",
    "        if val is not None:\n",
    "            d_stat[eid] = val\n",
    "    return d_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_maps(data_df: pd.DataFrame, top_dt, end_dt):\n",
    "    evmap = EventDefinitionMap(top_dt=top_dt, end_dt=end_dt)\n",
    "    evdict = {} # Event id -> list(datetime.datetime)\n",
    "    for _, row in data_df.iterrows():\n",
    "        row_eids = evmap.process_row(data_df.columns, row)\n",
    "        map(lambda eid: evdict.setdefault(eid, []).append(row['@timestamp']), row_eids)\n",
    "    return evmap, evdict\n",
    "\n",
    "def get_date_range(data_df):\n",
    "    date_format = '%Y-%m-%dT%H:%M:%S.%f000%z'\n",
    "    data_df['@timestamp'] = data_df['@timestamp'].apply(lambda x: datetime.datetime.strptime(x, date_format))\n",
    "    min_dt = data_df['@timestamp'].iloc[0].to_pydatetime()\n",
    "    max_dt = data_df['@timestamp'].iloc[-1].to_pydatetime()\n",
    "    print(f\"Min: {min_dt}\")\n",
    "    print(f\"Max: {max_dt}\")\n",
    "    top_dt = datetime.datetime.combine(min_dt.date(), datetime.time(hour=min_dt.hour)).replace(tzinfo=min_dt.tzinfo)\n",
    "    end_dt = datetime.datetime.combine(max_dt.date(), datetime.time(hour=max_dt.hour, minute=max_dt.minute + 1)).replace(tzinfo=min_dt.tzinfo)\n",
    "    return top_dt, end_dt\n",
    "\n",
    "\n",
    "def generate_transformation_knowledge(huawei_df: pd.DataFrame):\n",
    "    data_df = huawei_df.copy(deep=True)\n",
    "    data_df.drop(labels=[x for x in data_df.columns if x not in relevant_columns], axis=1, inplace=True)\n",
    "    data_df = data_df.sort_values(by='@timestamp').reset_index(drop=True)\n",
    "    top_dt, end_dt = get_date_range(data_df)\n",
    "    dur = datetime.timedelta(seconds=10)\n",
    "    evmap, evdict = create_maps(data_df, top_dt, end_dt)\n",
    "    data = event2stat(evdict, top_dt, end_dt, dur)\n",
    "    dm = np.array([d for eid, d in sorted(data.items())]).transpose()\n",
    "    df = pd.DataFrame(dm)\n",
    "    graph_algo = PC(CItest=\"binary\", method_indep=\"binary\")\n",
    "    output: nx.Graph = graph_algo.predict(df)\n",
    "    causality_records = []\n",
    "    for edge in list(output.edges()): # list((eid, eid))\n",
    "        from_eid, to_eid = edge[0], edge[1]\n",
    "        from_evdef = evmap.get_evdef(from_eid)\n",
    "        to_evdef = evmap.get_evdef(to_eid)\n",
    "        causality_records.append(\n",
    "            {\n",
    "                \"parent_id\": from_evdef.type + '#' + str(from_evdef.value),\n",
    "                \"parent_name\": from_evdef.value,\n",
    "                \"child_id\": to_evdef.type + '#' + str(to_evdef.value),\n",
    "                \"child_name\": to_evdef.value,\n",
    "            }\n",
    "        )\n",
    "    print(f\"Number of nodes: {output.number_of_nodes()}\")\n",
    "    print(f\"Number of edges: {output.number_of_edges()}\")\n",
    "    return pd.DataFrame.from_records(causality_records).drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating for size 100\n",
      "Min: 2019-11-19 17:00:05+01:00\n",
      "Max: 2019-11-19 17:04:20.472000+01:00\n",
      "0.12584590911865234\n",
      "Calculating for size 1000\n",
      "Min: 2019-11-19 17:00:05+01:00\n",
      "Max: 2019-11-19 17:17:31.496000+01:00\n",
      "0.7679634094238281\n",
      "Calculating for size 10000\n",
      "Min: 2019-11-19 17:00:05+01:00\n",
      "Max: 2019-11-19 21:03:02.049000+01:00\n",
      "9.66567349433899\n",
      "Calculating for size 50000\n",
      "Min: 2019-11-19 17:00:05+01:00\n",
      "Max: 2019-11-20 03:44:23.742000+01:00\n",
      "238.5599546432495\n"
     ]
    }
   ],
   "source": [
    "results['transformation no filter'] = []\n",
    "for i, huawei_df in enumerate(huawei_dfs):\n",
    "    start = time.time()\n",
    "    print(f\"Calculating for size {df_sizes[i]}\")\n",
    "    results['transformation no filter'].append(generate_transformation_knowledge(huawei_df))\n",
    "    print(time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heuristic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.features import sequences\n",
    "from collections import Counter\n",
    "\n",
    "causality_preprocessor = preprocessing.ConcurrentAggregatedLogsCausalityPreprocessor(huawei_config)\n",
    "huawei_config.min_causality = 0.01\n",
    "\n",
    "def collect_sequence_metadata(grouped_data):\n",
    "    transformer = sequences.load_sequence_transformer()\n",
    "    return transformer.collect_metadata(grouped_data, \"all_events\")\n",
    "\n",
    "def generate_heuristic_knowledge(huawei_df: pd.DataFrame):\n",
    "    log_only_data = huawei_df.copy(deep=True)\n",
    "    log_only_data[\"grouper\"] = 1\n",
    "    grouped_data = preprocessor._aggregate_per(log_only_data, aggregation_column=\"grouper\")\n",
    "    metadata = collect_sequence_metadata(grouped_data)\n",
    "\n",
    "    relevant_columns = set(\n",
    "        [\n",
    "            x\n",
    "            for x in preprocessor.relevant_columns\n",
    "            if not causality_preprocessor.config.log_only_causality or \"log\" in x\n",
    "        ]\n",
    "    )\n",
    "    counted_causality = causality_preprocessor._generate_counted_causality(\n",
    "        huawei_df, relevant_columns\n",
    "    )\n",
    "\n",
    "    causality_records = []\n",
    "    for from_value, to_values in counted_causality.items():\n",
    "        total_to_counts = len(to_values)\n",
    "        to_values_counter = Counter(to_values)\n",
    "        for to_value, to_count in to_values_counter.items():\n",
    "            if to_count / total_to_counts > causality_preprocessor.config.min_causality:\n",
    "                causality_records.append(\n",
    "                    {\n",
    "                        \"parent_id\": from_value,\n",
    "                        \"parent_name\": from_value.split(\"#\")[1],\n",
    "                        \"child_id\": to_value,\n",
    "                        \"child_name\": to_value.split(\"#\")[1],\n",
    "                    },\n",
    "                )\n",
    "\n",
    "    return (\n",
    "        pd.DataFrame.from_records(causality_records)\n",
    "        .drop_duplicates()\n",
    "        .reset_index(drop=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['heuristic'] = [generate_heuristic_knowledge(x) for x in huawei_dfs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_my_method_knoweldge(huawei_df):\n",
    "    vocab = set()\n",
    "    for _, row in huawei_df.iterrows():\n",
    "        for column in relevant_columns:\n",
    "            if column == '@timestamp':\n",
    "                continue\n",
    "            name = column + \"#\" + str(row[column]).lower()\n",
    "            if name not in vocab and row[column] != \"\":\n",
    "                vocab.add(name)\n",
    "    df_dict = {}\n",
    "    for column in vocab:\n",
    "        attribute, value = tuple(column.split('#'))\n",
    "        if column not in df_dict:\n",
    "            df_dict[column] = huawei_df[attribute].apply(lambda x: 1 if x == value else 0)\n",
    "        else:\n",
    "            df_dict[column] |= huawei_df[attribute].apply(lambda x: 1 if x == value else 0)\n",
    "    alg_df = pd.concat(df_dict, axis=1)\n",
    "    obj = PC(CItest=\"binary\", method_indep=\"binary\")\n",
    "    output = obj.predict(alg_df)\n",
    "    causality_records = []\n",
    "    for edge in list(output.edges()):\n",
    "        from_value, to_value = edge\n",
    "        try:\n",
    "            causality_records.append(\n",
    "                {\n",
    "                    \"parent_id\": from_value,\n",
    "                    \"parent_name\": from_value.split(\"#\")[1],\n",
    "                    \"child_id\": to_value,\n",
    "                    \"child_name\": to_value.split(\"#\")[1],\n",
    "                },\n",
    "            )\n",
    "        except Exception:\n",
    "            print(from_value)\n",
    "    return pd.DataFrame.from_records(causality_records).drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['my method'] = []\n",
    "for i, huawei_df in enumerate(huawei_dfs):\n",
    "    start = time.time()\n",
    "    print(f\"Calculating for size {df_sizes[i]}\")\n",
    "    results['my method'].append(generate_my_method_knoweldge(huawei_df))\n",
    "    print(time.time() - start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('healthcare-aiops')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6855f63c191af6fd2fd1da4b19dd7ae7c65708b4b78f6dbf4a71b45dd37ea344"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
