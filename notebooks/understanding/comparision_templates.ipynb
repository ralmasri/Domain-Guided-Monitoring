{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU automatically detected. Setting SETTINGS.GPU to 0, and SETTINGS.NJOBS to cpu_count.\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "import os\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from pathlib import Path\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = \"2\"\n",
    "\n",
    "import cdt\n",
    "cdt.SETTINGS.rpath = \"/usr/bin/Rscript\"\n",
    "from cdt.causality.graph import PC, GES\n",
    "\n",
    "import pandas as pd\n",
    "from src.features import preprocessing\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path('/home/ralmasri/projects/Thesis/Domain-Guided-Monitoring/data/')\n",
    "csv_path = data_path / \"logs_aggregated_concurrent.csv\"\n",
    "huawei_config = preprocessing.HuaweiPreprocessorConfig()\n",
    "huawei_config.aggregated_log_file = csv_path\n",
    "preprocessor = preprocessing.ConcurrentAggregatedLogsPreprocessor(huawei_config)\n",
    "# huawei_df = preprocessor._load_log_only_data().fillna(\"\")\n",
    "huawei_df = pd.read_csv(data_path / \"log_only_data.csv\").fillna(\"\").astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sizes = [100, 1000]\n",
    "huawei_dfs = [huawei_df.head(x) for x in df_sizes]\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_causality_records_to_graph(causality_record):\n",
    "    vocab = {}\n",
    "    index =  0\n",
    "    df = pd.DataFrame.from_records(causality_record).drop_duplicates().reset_index(drop=True)\n",
    "    df.drop(labels=[x for x in df.columns if x not in ['parent_id', 'child_id']], axis=1, inplace=True)\n",
    "    nodes = []\n",
    "    edges = []\n",
    "    for _, row in df.iterrows():\n",
    "        parent, child = row['parent_id'], row['child_id']\n",
    "        if parent not in vocab:\n",
    "            vocab[parent] = index\n",
    "            parent_type, parent_value = tuple(parent.split('#'))\n",
    "            nodes.append((index, {'value': parent_value, 'type': parent_type}))\n",
    "            index += 1\n",
    "        if child not in vocab:\n",
    "            vocab[child] = index\n",
    "            child_type, child_value = tuple(child.split('#'))\n",
    "            nodes.append((index, {'value': child_value, 'type': child_type}))\n",
    "            index += 1\n",
    "        edges.append((vocab[parent], vocab[child]))\n",
    "    G = nx.DiGraph()\n",
    "    G.add_nodes_from(nodes)\n",
    "    G.add_edges_from(edges)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CDT_ALGORITHMS = {\n",
    "    'constraint': lambda df: PC(CItest = 'binary').predict(df),\n",
    "    'score': lambda df: GES().predict(df),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation without filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "EvDef = namedtuple('EvDef', ['type', 'value'])\n",
    "class EventDefinitionMap: # eid -> evdef\n",
    "    def __init__(self, top_dt, end_dt):\n",
    "        self.top_dt = top_dt\n",
    "        self.end_dt = end_dt\n",
    "        self._emap = {} # key : eid, val : evdef\n",
    "        self._ermap = {} # key : evdef, val : eid\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._emap)\n",
    "\n",
    "    def _eids(self):\n",
    "        return self._emap.keys()\n",
    "\n",
    "    def _next_eid(self):\n",
    "        eid = len(self._emap)\n",
    "        while eid in self._emap:\n",
    "            eid += 1\n",
    "        else:\n",
    "            return eid\n",
    "\n",
    "    def get_evdef(self, eid) -> EvDef:\n",
    "        return self._emap[eid]\n",
    "\n",
    "    def get_eid(self, evdef):\n",
    "        return self._ermap[evdef]\n",
    "\n",
    "    def process_row(self, columns, row):\n",
    "        row_eids = []\n",
    "        for column in columns:\n",
    "                if column == '@timestamp':\n",
    "                    continue\n",
    "                value = row[column]\n",
    "                if value == \"\":\n",
    "                    continue\n",
    "                d = {\n",
    "                    \"type\": column,\n",
    "                    \"value\": row[column],\n",
    "                }\n",
    "\n",
    "                evdef = EvDef(**d)\n",
    "\n",
    "                if evdef in self._ermap:\n",
    "                    row_eids.append(self._ermap[evdef])\n",
    "                else:\n",
    "                    eid = self._next_eid()\n",
    "                    self._emap[eid] = evdef\n",
    "                    self._ermap[evdef] = eid\n",
    "                    row_eids.append(eid)\n",
    "        return row_eids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label(dt_range, duration):\n",
    "    top_dt, end_dt = dt_range\n",
    "    l_label = []\n",
    "    temp_dt = top_dt\n",
    "    while temp_dt < end_dt:\n",
    "        l_label.append(temp_dt)\n",
    "        temp_dt += duration\n",
    "    l_label.append(end_dt)\n",
    "    return l_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize(l_dt, l_label, method):\n",
    "\n",
    "    def return_empty(size):\n",
    "        if method in (\"count\", \"binary\"):\n",
    "            return [0] * bin_num\n",
    "        elif method == \"datetime\":\n",
    "            return [[] for i in range(bin_num)]\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                \"Invalid method name ({0})\".format(method))\n",
    "\n",
    "    def init_tempobj():\n",
    "        if method == \"count\":\n",
    "            return 0\n",
    "        elif method == \"binary\":\n",
    "            return 0\n",
    "        elif method == \"datetime\":\n",
    "            return []\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                \"Invalid method name ({0})\".format(method))\n",
    "\n",
    "    def update_tempobj():\n",
    "        if method == \"count\":\n",
    "            return temp + 1\n",
    "        elif method == \"binary\":\n",
    "            return 1\n",
    "        elif method == \"datetime\":\n",
    "            temp.append(new_dt)\n",
    "            return temp\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                \"Invalid method name ({0})\".format(method))\n",
    "\n",
    "    bin_num = len(l_label) - 1\n",
    "    l_dt_temp = sorted(l_dt)\n",
    "    if len(l_dt_temp) <= 0:\n",
    "        return_empty(bin_num)\n",
    "\n",
    "    iterobj = iter(l_dt_temp)\n",
    "    try:\n",
    "        new_dt = next(iterobj)\n",
    "    except StopIteration:\n",
    "        raise ValueError(\"Not empty list, but failed to get initial value\")\n",
    "    while new_dt < l_label[0]:\n",
    "        try:\n",
    "            new_dt = next(iterobj)\n",
    "        except StopIteration:\n",
    "            return_empty(bin_num)\n",
    "\n",
    "    ret = []\n",
    "    stop = False\n",
    "    for label_dt in l_label[1:]:\n",
    "        temp = init_tempobj()\n",
    "        if stop:\n",
    "            ret.append(temp)\n",
    "            continue\n",
    "        while new_dt < label_dt:\n",
    "            temp = update_tempobj()\n",
    "            try:\n",
    "                new_dt = next(iterobj)\n",
    "            except StopIteration:\n",
    "                # \"stop\" make data after label term be ignored\n",
    "                stop = True\n",
    "                break\n",
    "        ret.append(temp)\n",
    "    return ret\n",
    "\n",
    "def autodiscretize_with_slide(l_dt, binsize, slide, dt_range):\n",
    "    top_dt, end_dt = dt_range\n",
    "    slide_width = max(int(binsize.total_seconds() / slide.total_seconds()), 1)\n",
    "    l_top = label((top_dt, end_dt), slide)[:-1]\n",
    "    l_end = [min(t + binsize, end_dt) for t in l_top]\n",
    "\n",
    "    ret = []\n",
    "    noslide = discretize(l_dt, l_top + [end_dt], method='datetime')\n",
    "    for i, bin_end in enumerate(l_end):\n",
    "        l_dt_temp = []\n",
    "        for b in noslide[i:i+slide_width]:\n",
    "            l_dt_temp.extend([dt for dt in b if dt <= bin_end])\n",
    "    \n",
    "        if len(l_dt_temp) > 0:\n",
    "            ret.append(1)\n",
    "        else:\n",
    "            ret.append(0)\n",
    "\n",
    "    return ret\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume we're using the G-squared test so everything is binary\n",
    "def event2stat(evdict, top_dt, end_dt, dur, bin_overlap):\n",
    "    \"\"\"This function looks at each event and returns a map that maps the \n",
    "    eid to a list that describes in which time bins the event occurs\"\"\"\n",
    "    d_stat = {}\n",
    "    labels = label((top_dt, end_dt), dur)\n",
    "    for eid, l_ev in evdict.items():\n",
    "        # if len(l_ev) == 0: # Skip events that dont have timestamps (shouldn't be possible)\n",
    "        #     continue\n",
    "\n",
    "        if bin_overlap == datetime.timedelta(seconds = 0):\n",
    "            val = discretize(l_ev, labels, method=\"binary\")\n",
    "        else:\n",
    "            slide = dur - bin_overlap\n",
    "            val = autodiscretize_with_slide(l_ev, dur, slide, dt_range = (top_dt, end_dt))\n",
    "        if val is not None:\n",
    "            d_stat[eid] = val\n",
    "    return d_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_maps(data_df: pd.DataFrame, top_dt: datetime.datetime, end_dt: datetime.datetime):\n",
    "    def append_to_evdict(evdict, eid, val):\n",
    "        if eid in evdict:\n",
    "                evdict[eid].append(val)\n",
    "        else:\n",
    "            evdict[eid] = [val]\n",
    "\n",
    "    evmap = EventDefinitionMap(top_dt=top_dt, end_dt=end_dt)\n",
    "    evdict = {} # Event id -> list(datetime.datetime)\n",
    "    for _, row in data_df.iterrows():\n",
    "        row_eids = evmap.process_row(data_df.columns, row)\n",
    "        timestamp = row['@timestamp']\n",
    "        for eid in row_eids:\n",
    "            if eid in evdict:\n",
    "                evdict[eid].append(row['@timestamp'])\n",
    "            else:\n",
    "                evdict[eid] = [row['@timestamp']]            \n",
    "    return evmap, evdict\n",
    "\n",
    "def get_date_range(data_df):\n",
    "    date_format = '%Y-%m-%dT%H:%M:%S.%f000%z'\n",
    "    data_df['@timestamp'] = data_df['@timestamp'].apply(lambda x: datetime.datetime.strptime(x, date_format))\n",
    "    min_dt = data_df['@timestamp'].iloc[0].to_pydatetime()\n",
    "    max_dt = data_df['@timestamp'].iloc[-1].to_pydatetime()\n",
    "    print(f\"Min: {min_dt}\")\n",
    "    print(f\"Max: {max_dt}\")\n",
    "    top_dt = datetime.datetime.combine(min_dt.date(), datetime.time(hour=min_dt.hour)).replace(tzinfo=min_dt.tzinfo)\n",
    "    end_dt = datetime.datetime.combine(max_dt.date(), datetime.time(hour=max_dt.hour, minute=max_dt.minute + 1)).replace(tzinfo=min_dt.tzinfo)\n",
    "    return top_dt, end_dt\n",
    "\n",
    "\n",
    "def generate_transformation_knowledge(huawei_df: pd.DataFrame, relevant_columns, method='constraint'):\n",
    "    start_time = time.time()\n",
    "    data_df = huawei_df.copy(deep=True)\n",
    "    data_df.drop(labels=[x for x in data_df.columns if  x != '@timestamp' and x not in relevant_columns], axis=1, inplace=True)\n",
    "    data_df = data_df.sort_values(by='@timestamp').reset_index(drop=True).fillna(\"\")\n",
    "    top_dt, end_dt = get_date_range(data_df)\n",
    "    dur = datetime.timedelta(seconds=10)\n",
    "    bin_overlap = datetime.timedelta(seconds = 8)\n",
    "    evmap, evdict = create_maps(data_df, top_dt, end_dt)\n",
    "    data = event2stat(evdict, top_dt, end_dt, dur, bin_overlap)\n",
    "    dm = np.array([d for eid, d in sorted(data.items())]).transpose()\n",
    "    df = pd.DataFrame(dm)\n",
    "    print(f\"Preprocessing time in seconds: {time.time() - start_time}\")\n",
    "    output: nx.Graph = CDT_ALGORITHMS[method](df)\n",
    "    print(f\"Number of nodes (including non-connected nodes): {output.number_of_nodes()}\")\n",
    "    causality_records = []\n",
    "    for edge in list(output.edges()): # list((eid, eid))\n",
    "        from_eid, to_eid = edge[0], edge[1]\n",
    "        from_evdef = evmap.get_evdef(from_eid)\n",
    "        to_evdef = evmap.get_evdef(to_eid)\n",
    "        causality_records.append(\n",
    "            {\n",
    "                \"parent_id\": from_evdef.type + '#' + str(from_evdef.value),\n",
    "                \"parent_name\": from_evdef.value,\n",
    "                \"child_id\": to_evdef.type + '#' + str(to_evdef.value),\n",
    "                \"child_name\": to_evdef.value,\n",
    "            }\n",
    "        )\n",
    "    graph = convert_causality_records_to_graph(causality_records)\n",
    "    print(graph)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constraint-based:\n",
      "Calculating for size 100\n",
      "Min: 2019-11-19 17:00:05+01:00\n",
      "Max: 2019-11-19 17:04:20.472000+01:00\n",
      "Preprocessing time in seconds: 0.11882805824279785\n",
      "Number of nodes (including non-connected nodes): 29\n",
      "DiGraph with 14 nodes and 16 edges\n",
      "Min: 2019-11-19 17:00:05+01:00\n",
      "Max: 2019-11-19 17:04:20.472000+01:00\n",
      "Preprocessing time in seconds: 0.12053751945495605\n",
      "Number of nodes (including non-connected nodes): 18\n",
      "DiGraph with 15 nodes and 18 edges\n",
      "Min: 2019-11-19 17:00:05+01:00\n",
      "Max: 2019-11-19 17:04:20.472000+01:00\n",
      "Preprocessing time in seconds: 0.1358027458190918\n",
      "Number of nodes (including non-connected nodes): 47\n",
      "DiGraph with 28 nodes and 30 edges\n",
      "Total processing time in seconds: 42.2705819606781\n",
      "Calculating for size 1000\n",
      "Min: 2019-11-19 17:00:05+01:00\n",
      "Max: 2019-11-19 17:17:31.496000+01:00\n",
      "Preprocessing time in seconds: 0.9307460784912109\n",
      "Number of nodes (including non-connected nodes): 69\n",
      "DiGraph with 60 nodes and 98 edges\n",
      "Min: 2019-11-19 17:00:05+01:00\n",
      "Max: 2019-11-19 17:17:31.496000+01:00\n",
      "Preprocessing time in seconds: 1.2083144187927246\n",
      "Number of nodes (including non-connected nodes): 18\n",
      "DiGraph with 16 nodes and 20 edges\n",
      "Min: 2019-11-19 17:00:05+01:00\n",
      "Max: 2019-11-19 17:17:31.496000+01:00\n",
      "Preprocessing time in seconds: 0.8193447589874268\n",
      "Number of nodes (including non-connected nodes): 87\n",
      "DiGraph with 73 nodes and 102 edges\n",
      "Total processing time in seconds: 65.48097133636475\n",
      "##################################################\n",
      "Score-based:\n",
      "Calculating for size 100\n",
      "Min: 2019-11-19 17:00:05+01:00\n",
      "Max: 2019-11-19 17:04:20.472000+01:00\n",
      "Preprocessing time in seconds: 0.10930371284484863\n",
      "Number of nodes (including non-connected nodes): 29\n",
      "DiGraph with 29 nodes and 70 edges\n",
      "Min: 2019-11-19 17:00:05+01:00\n",
      "Max: 2019-11-19 17:04:20.472000+01:00\n",
      "Preprocessing time in seconds: 0.07581114768981934\n",
      "Number of nodes (including non-connected nodes): 18\n",
      "DiGraph with 18 nodes and 35 edges\n",
      "Min: 2019-11-19 17:00:05+01:00\n",
      "Max: 2019-11-19 17:04:20.472000+01:00\n",
      "Preprocessing time in seconds: 0.2959001064300537\n",
      "Number of nodes (including non-connected nodes): 47\n",
      "DiGraph with 47 nodes and 122 edges\n",
      "Total processing time in seconds: 10.943166017532349\n",
      "Calculating for size 1000\n",
      "Min: 2019-11-19 17:00:05+01:00\n",
      "Max: 2019-11-19 17:17:31.496000+01:00\n",
      "Preprocessing time in seconds: 0.7051565647125244\n",
      "Number of nodes (including non-connected nodes): 69\n",
      "DiGraph with 69 nodes and 200 edges\n",
      "Min: 2019-11-19 17:00:05+01:00\n",
      "Max: 2019-11-19 17:17:31.496000+01:00\n",
      "Preprocessing time in seconds: 0.554358959197998\n",
      "Number of nodes (including non-connected nodes): 18\n",
      "DiGraph with 18 nodes and 31 edges\n",
      "Min: 2019-11-19 17:00:05+01:00\n",
      "Max: 2019-11-19 17:17:31.496000+01:00\n",
      "Preprocessing time in seconds: 0.8730781078338623\n",
      "Number of nodes (including non-connected nodes): 87\n",
      "DiGraph with 87 nodes and 271 edges\n",
      "Total processing time in seconds: 20.1776442527771\n"
     ]
    }
   ],
   "source": [
    "def calculate_knowledge_for_huawei_dfs(huawei_dfs, method, results):\n",
    "    results[f\"fine_{method}\"] = []\n",
    "    results[f\"coarse_{method}\"] = []\n",
    "    results[f\"fine+coarse_{method}\"] = []\n",
    "    for i, huawei_df in enumerate(huawei_dfs):\n",
    "        start = time.time()\n",
    "        print(f\"Calculating for size {df_sizes[i]}\")\n",
    "        results[f\"fine_{method}\"].append(generate_transformation_knowledge(huawei_df, ['fine_log_cluster_template'], method))\n",
    "        results[f\"coarse_{method}\"].append(generate_transformation_knowledge(huawei_df, ['coarse_log_cluster_template'], method))\n",
    "        results[f\"fine+coarse_{method}\"].append(generate_transformation_knowledge(huawei_df, ['fine_log_cluster_template', 'coarse_log_cluster_template'], method))\n",
    "        print(f\"Total processing time in seconds: {time.time() - start}\")\n",
    "\n",
    "print(\"Constraint-based:\")\n",
    "calculate_knowledge_for_huawei_dfs(huawei_dfs, 'constraint', results)\n",
    "print('##################################################')\n",
    "print(\"Score-based:\")\n",
    "calculate_knowledge_for_huawei_dfs(huawei_dfs, 'score', results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_my_method_knoweldge(huawei_df, method, relevant_columns):\n",
    "    start_time = time.time()\n",
    "    vocab = set()\n",
    "    for _, row in huawei_df.iterrows():\n",
    "        for column in relevant_columns:\n",
    "            if column == '@timestamp':\n",
    "                continue\n",
    "            name = column + \"#\" + str(row[column]).lower()\n",
    "            if name not in vocab and row[column] != \"\":\n",
    "                vocab.add(name)\n",
    "    df_dict = {}\n",
    "    for column in vocab:\n",
    "        attribute, value = tuple(column.split('#'))\n",
    "        if column not in df_dict:\n",
    "            df_dict[column] = huawei_df[attribute].apply(lambda x: 1 if x == value else 0)\n",
    "        else:\n",
    "            df_dict[column] |= huawei_df[attribute].apply(lambda x: 1 if x == value else 0)\n",
    "    alg_df = pd.concat(df_dict, axis=1)\n",
    "    print(f\"Preprocessing time in seconds: {time.time() - start_time}\")\n",
    "    output: nx.Graph = CDT_ALGORITHMS[method](alg_df)\n",
    "    print(f\"Number of nodes (including non-connected nodes): {output.number_of_nodes()}\")\n",
    "    causality_records = []\n",
    "    for edge in list(output.edges()):\n",
    "        from_value, to_value = edge\n",
    "        causality_records.append(\n",
    "            {\n",
    "                \"parent_id\": from_value,\n",
    "                \"parent_name\": from_value.split(\"#\")[1],\n",
    "                \"child_id\": to_value,\n",
    "                \"child_name\": to_value.split(\"#\")[1],\n",
    "            },\n",
    "        )\n",
    "    \n",
    "    graph = convert_causality_records_to_graph(causality_records)\n",
    "    print(graph)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constraint-based:\n",
      "Calculating for size 100\n",
      "Preprocessing time in seconds: 0.09572410583496094\n",
      "Number of nodes (including non-connected nodes): 29\n",
      "DiGraph with 0 nodes and 0 edges\n",
      "Preprocessing time in seconds: 0.05378103256225586\n",
      "Number of nodes (including non-connected nodes): 18\n",
      "DiGraph with 2 nodes and 2 edges\n",
      "Preprocessing time in seconds: 0.05040597915649414\n",
      "Number of nodes (including non-connected nodes): 47\n",
      "DiGraph with 38 nodes and 40 edges\n",
      "Total processing time in seconds: 21.263933420181274\n",
      "Calculating for size 1000\n",
      "Preprocessing time in seconds: 0.2516171932220459\n",
      "Number of nodes (including non-connected nodes): 69\n",
      "DiGraph with 6 nodes and 24 edges\n",
      "Preprocessing time in seconds: 0.15404820442199707\n",
      "Number of nodes (including non-connected nodes): 18\n",
      "DiGraph with 7 nodes and 31 edges\n",
      "Preprocessing time in seconds: 0.2596909999847412\n",
      "Number of nodes (including non-connected nodes): 87\n",
      "DiGraph with 37 nodes and 36 edges\n",
      "Total processing time in seconds: 45.06828784942627\n",
      "##################################################\n",
      "Score-based:\n",
      "Calculating for size 100\n",
      "Preprocessing time in seconds: 0.04058575630187988\n",
      "Number of nodes (including non-connected nodes): 29\n",
      "DiGraph with 0 nodes and 0 edges\n",
      "Preprocessing time in seconds: 0.03388237953186035\n",
      "Number of nodes (including non-connected nodes): 18\n",
      "DiGraph with 18 nodes and 17 edges\n",
      "Preprocessing time in seconds: 0.05387163162231445\n",
      "Number of nodes (including non-connected nodes): 47\n",
      "DiGraph with 42 nodes and 51 edges\n",
      "Total processing time in seconds: 7.845319986343384\n",
      "Calculating for size 1000\n",
      "Preprocessing time in seconds: 0.3048100471496582\n",
      "Number of nodes (including non-connected nodes): 69\n",
      "DiGraph with 68 nodes and 79 edges\n",
      "Preprocessing time in seconds: 0.20238494873046875\n",
      "Number of nodes (including non-connected nodes): 18\n",
      "DiGraph with 18 nodes and 29 edges\n",
      "Preprocessing time in seconds: 0.5630607604980469\n",
      "Number of nodes (including non-connected nodes): 87\n",
      "DiGraph with 84 nodes and 150 edges\n",
      "Total processing time in seconds: 12.6784987449646\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def calculate_knowledge_for_huawei_dfs_my_method(huawei_dfs, method, results):\n",
    "    results[f\"mm_fine_{method}\"] = []\n",
    "    results[f\"mm_coarse_{method}\"] = []\n",
    "    results[f\"mm_fine+coarse_{method}\"] = []\n",
    "    for i, huawei_df in enumerate(huawei_dfs):\n",
    "        start = time.time()\n",
    "        print(f\"Calculating for size {df_sizes[i]}\")\n",
    "        results[f\"mm_fine_{method}\"].append(generate_my_method_knoweldge(huawei_df, method, ['fine_log_cluster_template']))\n",
    "        results[f\"mm_coarse_{method}\"].append(generate_my_method_knoweldge(huawei_df, method, ['coarse_log_cluster_template']))\n",
    "        results[f\"mm_fine+coarse_{method}\"].append(generate_my_method_knoweldge(huawei_df, method, ['fine_log_cluster_template', 'coarse_log_cluster_template']))\n",
    "        print(f\"Total processing time in seconds: {time.time() - start}\")\n",
    "\n",
    "print(\"Constraint-based:\")\n",
    "calculate_knowledge_for_huawei_dfs_my_method(huawei_dfs, 'constraint', results)\n",
    "print('##################################################')\n",
    "print(\"Score-based:\")\n",
    "calculate_knowledge_for_huawei_dfs_my_method(huawei_dfs, 'score', results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from bokeh.io import output_notebook, show, save\n",
    "\n",
    "COLOR_MAP = {\n",
    "    'Hostname': '#FF0000', # red\n",
    "    'log_level': '#0000FF', # blue\n",
    "    'programname': '#00FF00', # green\n",
    "    'python_module': '#FFFF00', # yellow\n",
    "    'http_status': '#FFA500', # orange\n",
    "    'http_method': '#FFC0CB', # pink\n",
    "    'fine_log_cluster_template': '#A020F0', # purple\n",
    "    'coarse_log_cluster_template': '#808080', # grey\n",
    "    'url_cluster_template': '#964B00' # brown\n",
    "}\n",
    "\n",
    "def get_color_map(G: nx.Graph):\n",
    "    return [COLOR_MAP[node[1]['type']] for node in G.nodes(data=True)]\n",
    "\n",
    "def node_to_color(node):\n",
    "    return COLOR_MAP[node[1]['type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\nconst JS_MIME_TYPE = 'application/javascript';\n  const HTML_MIME_TYPE = 'text/html';\n  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  const CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    const script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    const cell = handle.cell;\n\n    const id = cell.output_area._bokeh_element_id;\n    const server_id = cell.output_area._bokeh_server_id;\n    // Clean up Bokeh references\n    if (id != null && id in Bokeh.index) {\n      Bokeh.index[id].model.document.clear();\n      delete Bokeh.index[id];\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd_clean, {\n        iopub: {\n          output: function(msg) {\n            const id = msg.content.text.trim();\n            if (id in Bokeh.index) {\n              Bokeh.index[id].model.document.clear();\n              delete Bokeh.index[id];\n            }\n          }\n        }\n      });\n      // Destroy server and session\n      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd_destroy);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    const output_area = handle.output_area;\n    const output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n      return\n    }\n\n    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      const bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      const script_attrs = bk_div.children[0].attributes;\n      for (let i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      const toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    const events = require('base/js/events');\n    const OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    const el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-2.4.3.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n          for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\nif (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));",
      "application/vnd.bokehjs_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_notebook() # allows bokeh visualizations to be shown in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for G_list in results.values():\n",
    "    for G in G_list:\n",
    "        for node in G.nodes(data=True):\n",
    "            node[1]['color'] = node_to_color(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot.networkx as hvnx\n",
    "def plot(G, save_name):\n",
    "    pos =  nx.nx_agraph.graphviz_layout(G)\n",
    "    layout = hvnx.draw(G, pos, arrows=True, with_labels=True, node_color='color', height = 800, width=800)\n",
    "    hvnx.save(layout, f\"html/{save_name}.html\")\n",
    "    return layout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = results['fine_score'][0]\n",
    "save_name = 'fine_score_100'\n",
    "layout = plot(G, save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = results['coarse_score'][0]\n",
    "save_name = 'coarse_score_100'\n",
    "layout = plot(G, save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = results['fine+coarse_score'][0]\n",
    "save_name = 'fine+coarse_score_100'\n",
    "layout = plot(G, save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = results['fine_score'][1]\n",
    "save_name = 'fine_score_1000'\n",
    "layout = plot(G, save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = results['coarse_score'][1]\n",
    "save_name = 'coarse_score_1000'\n",
    "layout = plot(G, save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = results['fine+coarse_score'][1]\n",
    "save_name = 'fine+coarse_score_1000'\n",
    "layout = plot(G, save_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = results['fine_constraint'][0]\n",
    "save_name = 'fine_constraint_100'\n",
    "layout = plot(G, save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = results['coarse_constraint'][0]\n",
    "save_name = 'coarse_constraint_100'\n",
    "layout = plot(G, save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = results['fine+coarse_constraint'][0]\n",
    "save_name = 'fine+coarse_constraint_100'\n",
    "layout = plot(G, save_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit ('miniconda3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "46a3606ad50f253451469dd300eca0a6f34c2245337430b3b8f5104bce2d8ed7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
