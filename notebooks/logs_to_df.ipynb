{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the datasets were downloaded from [LogHub's Github page](https://github.com/logpai/loghub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_to_dataframe(log_file, regex, headers):\n",
    "    \"\"\" Function to transform log file to dataframe\n",
    "    \"\"\"\n",
    "    log_messages = []\n",
    "    with open(log_file, 'r', errors='ignore') as fin:\n",
    "        for line in fin.readlines():\n",
    "            try:\n",
    "                match = regex.search(line.strip())\n",
    "                message = [match.group(header) for header in headers]\n",
    "                log_messages.append(message)\n",
    "            except Exception as e:\n",
    "                # print(\"\\n\", line)\n",
    "                # print(e)\n",
    "                pass\n",
    "    logdf = pd.DataFrame(log_messages, columns=headers)\n",
    "    return logdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_logformat_regex(logformat):\n",
    "    \"\"\" Function to generate regular expression to split log messages\n",
    "    \"\"\"\n",
    "    headers = []\n",
    "    splitters = re.split(r'(<[^<>]+>)', logformat)\n",
    "    regex = ''\n",
    "    for k in range(len(splitters)):\n",
    "        if k % 2 == 0:\n",
    "            splitter = re.sub(' +', '\\\\\\s+', splitters[k])\n",
    "            regex += splitter\n",
    "        else:\n",
    "            header = splitters[k].strip('<').strip('>')\n",
    "            regex += '(?P<%s>.*?)' % header\n",
    "            headers.append(header)\n",
    "    regex = re.compile('^' + regex + '$')\n",
    "    return headers, regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, logName, log_format):\n",
    "    headers, regex = generate_logformat_regex(log_format)\n",
    "    df_log = log_to_dataframe(os.path.join(path, logName), regex, headers)\n",
    "    return df_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(df: pd.DataFrame, k: int):\n",
    "    nrows = range(df.shape[0])\n",
    "    ix = random.randint(nrows.start, nrows.stop - k)\n",
    "    return df.iloc[ix:(ix + k), :].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '' # Path of the input *.log files and output *.csv files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BGL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgl_format = '<Label> <Id> <Date> <Code1> <timestamp> <Code2> <Component1> <Component2> <Level> <Payload>'\n",
    "bgl_df = load_data(data_path, 'BGL.log', bgl_format)\n",
    "\n",
    "# Select sequence of size k randomly\n",
    "bgl_df = sample(bgl_df, 2000)\n",
    "bgl_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform timestamps\n",
    "bgl_ts_format = \"%Y-%m-%d-%H.%M.%S.%f\"\n",
    "bgl_df['timestamp'] = pd.to_datetime(bgl_df['timestamp'], format=bgl_ts_format)\n",
    "\n",
    "# Transform labels to binary\n",
    "bgl_df['Label'] = bgl_df['Label'].apply(lambda x: '0' if x == '-' else '1')\n",
    "bgl_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgl_df.groupby(['Label'])['Label'].count().rename({'1': 'Anomaly', '0': 'Normal'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgl_df.to_csv(os.path.join(data_path, 'BGL_2k.csv'), index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_format = '<Date> <Time> <Pid> <Level> <Component>: <Payload>'\n",
    "hdfs_df = load_data(data_path, 'HDFS.log', hdfs_format)\n",
    "\n",
    "# Select sequence of size k randomly\n",
    "hdfs_df = sample(hdfs_df, 2000)\n",
    "hdfs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since date and time are separate, we need to combine them into a timestamp\n",
    "hdfs_df['timestamp'] = hdfs_df['Date'] + '-' + hdfs_df['Time']\n",
    "hdfs_ts_format = '%y%m%d-%H%M%S'\n",
    "hdfs_df['timestamp'] = pd.to_datetime(hdfs_df['timestamp'], format=hdfs_ts_format)\n",
    "\n",
    "# Label information is also missing so we need add that\n",
    "anomaly_labels = pd.read_csv(os.path.join(data_path, 'anomaly_label.csv'))\n",
    "def anomaly_classification(payload):\n",
    "    blkId_list = re.findall(r'(blk_-?\\d+)', payload)\n",
    "    blkId_set = list(set(blkId_list))\n",
    "    if len(blkId_set) != 1: # This shouldn't happen\n",
    "        raise ValueError(f\"Row  has {len(blkId_set)} blkIds. Cannot determine if anomaly or not\")\n",
    "    blkId = blkId_set[0]\n",
    "    is_anomaly = anomaly_labels.loc[anomaly_labels['BlockId'] == blkId, 'Label'].tolist()[0] == 'Anomaly'\n",
    "    return '1' if is_anomaly else '0'\n",
    "\n",
    "hdfs_df['Label'] = hdfs_df['Payload'].apply(anomaly_classification)\n",
    "hdfs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_df.groupby(['Label'])['Label'].count().rename({'1': 'Anomaly', '0': 'Normal'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_df.to_csv(os.path.join(data_path, 'HDFS_2k.csv'), index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mozilla Thunderbird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbird_format = '<Label> <Id> <Date> <Admin> <Month> <Day> <Time> <AdminAddr> <Payload>'\n",
    "\n",
    "# The Thunderbird log file is really huge so I'm just going to use the Github's\n",
    "# 2k sample\n",
    "tbird_df = load_data(data_path, 'Thunderbird_2k.log', tbird_format)\n",
    "\n",
    "# Select sequence of size k randomly\n",
    "# tbird_df = sample(tbird_df, 2000)\n",
    "tbird_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform timestamps\n",
    "tbird_df['timestamp'] = tbird_df['Date'] + '-' + tbird_df['Time']\n",
    "tbird_ts_format = '%Y.%m.%d-%H:%M:%S'\n",
    "tbird_df['timestamp'] = pd.to_datetime(tbird_df['timestamp'], format=tbird_ts_format)\n",
    "\n",
    "# Transform labels\n",
    "tbird_df['Label'] = tbird_df['Label'].apply(lambda x: '0' if x == '-' else '1')\n",
    "tbird_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbird_df.groupby(['Label'])['Label'].count().rename({'1': 'Anomaly', '0': 'Normal'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbird_df.to_csv(os.path.join(data_path, 'Thunderbird_2k.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "afb734500600fd355917ca529030176ea0ca205570884b88f2f6f7d791fd3fbe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
